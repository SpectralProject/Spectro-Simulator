# Web Frontend for a Spectro Machine

## Structure
- pages/bios contains the view of the bios
- pages/arcboot contains the view of the bootloader. By default, fast boot is enabled which skips the bootloader visuals
- the WindowManager component should handle all the open windows. Windows change the value of show = 0, 1, 2 to close, minimise, show

### Bios

### Arcboot
A bootloader program. By default, exists in `/arcboot/boot.elf` in the boot drive. When formatted with qfs, usually stored near the start but doesnt have to be since the header entries/database should point to it as a `boot` directory.
Note the semantic fs is stored in `/semantic` in subdirs that are namespaced according to tags.
- For UEFI, the first stage Arcboot must be placed at the start of the drive. This happens when you format/wipe an entire drive as a qfs partition.
- The first stage bootloader has a pointer to the second stage bootloader stored elsewhere on disk. A linked list of pointers follows if the bootloader is too big.
- When the BIOS firmware finds the first bootable partition, it will boot that. By default, a spectro machine should have the bootable partition in drive0 formatted with GUID Partition Table. The `/arcboot` partition itself is formatted in FAT, which should appear in the first GUID entry.
- BIOS should be able to load the boot.elf image which loads the first stage bootloader.
- First stage bootloader does extra config but mostly can just pass to the second stage.
- Second stage should have unlimited power and be able to interface with the hardware in complex ways. It should be able to draw more graphics and animations as well as complex text rendering using its builtin graphics API that is also shared by a Quanta OS kernel img. Arcboot has its own set of drivers to do these things, and Quanta expands on them to do even more complex things and more efficiently.
- Second stage should find all the multiboot compliant kernels and list them. It finds them by looking at the GUID entries to see if there are any that match specific bits denoting that the partition stores the kernel img. Usually, this is placed in `/kernel` on a QFS partition. Well the entire QFS partition is the kernel img basically.
- User chooses a kernel img or one is chosen automatically. It is then transferred entirely into RAM and run sequentially. Kernel startup scripts and stuff can run here. Complex kernel drivers and services that rely on the bootloader's stuff are also loaded here.

### Kernel
- Once kernel code takes control, it is already placed in a paged environment set up by the bootloader. It can now setup the extra bookkeeping to manage that environment since the bootloader merely sets it up.
- Kernel should be able to load the filesystem view by checking the drives, `drive0, drive1`. Usually all the kernel scripts are placed in `drive0` under the kernel partition `/kernel/scripts`. These scripts are runtime scripts so they are either compiled then run or interpreted. It is possible to AOT compile them and run as a prebuilt executable in `/kernel/scripts/bin`. The kernel should look at a file, `/kernel/config.txt` to understand the order in which to execute the scripts.
- Kernel should then run the scripts. Usually, this means modesetting drivers, checking the list of devices passed by the bootloader, and loading appropriate drivers to manage them, starting up services that use those drivers like Arcgraphics, ArcWM, Networkd, IPCd, Arcvisor, Hardwared (hwd). Tools like top, hardware-details, firewall, rely on these services. Apps also rely on these services and tools.
- After services are loaded by `/kernel/scripts/arcstart`, startup apps and userspace config can be loaded. These usually rely on the DE, which should have been started by `arcstart`. The user list is loaded and the first one is shown. The user can then log in, which creates a main login shell with extra config in `/usr/config`. Stuff like `bash.rc, .profile` are mostly replaced with a key value store like Windows HKEY in `/usr/config/config.db` which can be interacted with via an ASCII editor as well as the builtin `arcconfig` command.
- The user config file should define things like date/time/locale, desktop preferences. Things like permissions and security are managed by the config databases in `/security/config`. In `/usr/config/scripts` we have a bunch of scripts to call in the terminal `arcterm` at login. Usually these involve exporting the `PATH` variable and other `ENV` stuff. The system `ENV` should be in `/sys/config`. Default profiles, preferences, etc are also stored there, and when you create a new user, you simply clone another `/usr/config` from `sys/config`.
- Graphical things like the desktop wallpaper and layout, apps on the dock, the apps a user has access to should all be in `/usr/config/gui`.
- Apps should be run by `run <path/to/app>`. `run` is a builtin command that tells the shell/kernel to spawn a new process. With a POSIX extension, you can instead call `<path/to/app>` directly which uses the posix interface instead. The POSIX interface uses `posix_spawn` to create a new process, although `fork` may also be used for older programs that rely on it. The Arc interface calls `spawn_process(program_path: String, options: Options)`, leaving an extensible Options key-value interface that can be modified in later versions. The process manager `ArcProcessManager` checks for specific key-val pairs to do certain things. If you wish to hook onto the process with your own code and scripts, you can modify `/process/config` for specific processes. By default, most program types have ways to do IPC (custom signals/slots), start, shutdown, background (pause), foreground (resume). The IPC mechanism is a bus. And if you want to make an App that accepts a certain signal from other apps/processes, then you can hook onto `/process/list/<app>` to add a new slot that accepts a certain type of signal. Similarly you can add a new signal for an app/program. All of these configuration are done by key-val database stores, though can also be accessed by ASCII editors, but may be slower unless you turn on `Enable Fast DB Text Editing` which basically duplicates the db as an ASCII file which you can freely modify. 
- For signals, your program should be able to hook onto the signal API via `signal_send(pid: u64, signal: Signal) -> Result<Status>`. Note the send and receive are async by default. If using rust, you can use the builtin `async/await` mechanism in core. In `std` you can use threads completely and specify more complex logic. When a signal is received, it should be acknowledged with a reply signal with `ACK`. This can technically be done at any time for the receiving process, although by default programs that use the ArcIPC API must register a slot via `register_slot(signal: Signal, callback: Function)`. If you wish to ignore a signal like `KILL`, you can do `register_slot(Signal::KILL, []() {})` to make the process do nothing. This overrides the defaults when you compile using `Arc` compiler.
- ArcIPC basically routes the signals to their respective `pid`. When you call `send_signal(pid, signal, info)`, you create a userspace `IPCInfo` buffer in userspace RAM and store and format your message there. The format is stored in the header of the message. Then you specify a pointer to that block of virtual memory (should be contiguous), and make an IPC syscall by directly calling the MMIO IPC function in `0x60000000`. You can get this address via the `ipc.so` which programs should link to at runtime (managed by ArcProcessManager which links it automatically to the program). BY putting the args in a0..2, you then make an async call to it and pass that into another thread. The kernel should be able to schedule a userprocess thread for it by calling `make_thread`. This should queue the thread in the running process' `thread_queue`. The MMIO function looks at its list of processes in `pid_table` and checks if there is a matching entry. If so, it continues and sends a request to the `pid` by making a userspace interrupt for that process. The kernel should pause execution of that process or queue a signal to that process' `signal_queue`. The process should automatically check its signal queue as its program is running. The process should then pause execution at its current point and create a thread that calls its signal handler function. To avoid redunancy, default  signal handlers are stored in RAM at `0x60001000` which the compiler should compile to by default. After the signal handler returns, the program can resume at its current instruction with any new information updated in its virtual memory space. A kservice makes high use of this to do certain things like create a new window, handle network requests, open a file from a filesystem. KServices are optimised to basically do hardly anything/basically nothing and simply handle requests. They usually dont need to be 'paused' either but it is usually fine. Just call `make_thread` and join as usual. The GUIs may need more thinking though as we want to update them frequently as possible. Maybe a clock can be used to force an update for the GUI at every few minutes. Certain things that are bound to affect the GUI should tell the GUI to update via an IPC call to the GUI process `pid = 5`, `signal = Signal::REFRESH_STATE` which tells the GUI to redraw using its new state updated by some other process. If the GUI would make changes to its own config and backend, it should have builtin code to refresh right after.

NOTE: Quantii usually refers to the userspace which is decoupled from the kernel code. Quantii code is a suite of stuff stored across drives in multiple partitions and filesystems. But the kernel code/img is usually stored in a single partition formatted with QFS. The semantic fs is a software based thing that gives a view of a sub HFS tree as an SFS interface. It is mostly useful as a GUI but can also be interacted with via shell. Quantii code includes the DE, SFS, Tools and Apps. Neutron contains code for services, drivers, filesystem, processes. The interface between Quantii and Neutron are usually managed by `Managers` like `ArcProc` that creates process containers for Quantii apps.
